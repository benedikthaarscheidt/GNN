{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ef7cdb-7fdf-4a76-b46f-c742b0bcb014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1.post106\n",
      "None\n",
      "True\n",
      "Memory allocated: 0.0 MB\n",
      "Max memory allocated: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"scripts\")\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Check if PyTorch is installed\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "print(torch.version.cuda)  # Should match 12.6 or similar\n",
    "print(torch.backends.cudnn.enabled)  # Sho\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import networkx as nx\n",
    "import scripts\n",
    "from scripts import *\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna\n",
    "import models\n",
    "from optuna.integration import TensorBoardCallback\n",
    "from model_GNN import ModularPathwayConv, ModularGNN\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "from model_ResNet import CombinedModel, ResNet, DrugMLP  \n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "print(f\"Max memory allocated: {torch.cuda.max_memory_allocated() / 1e6} MB\")\n",
    "import uuid\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3375c36-291a-4d3e-9559-df17fcab9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_data(n_fold=0, fp_radius=2):\n",
    "    \"\"\"Download, process, and prepare data for use in graph-based machine learning models.\"\"\"\n",
    "    import os\n",
    "    import zipfile\n",
    "    import requests\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from torch_geometric.data import Data\n",
    "    import scripts  # Assuming scripts has required functions\n",
    "\n",
    "    def download_if_not_present(url, filepath):\n",
    "        \"\"\"Download a file from a URL if it does not exist locally.\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"File not found at {filepath}. Downloading...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(\"Download completed.\")\n",
    "        else:\n",
    "            print(f\"File already exists at {filepath}.\")\n",
    "\n",
    "    # Step 1: Download and load RNA-seq data\n",
    "    zip_url = \"https://cog.sanger.ac.uk/cmp/download/rnaseq_all_20220624.zip\"\n",
    "    zip_filepath = \"data/rnaseq.zip\"\n",
    "    rnaseq_filepath = \"data/rnaseq_normcount.csv\"\n",
    "    if not os.path.exists(rnaseq_filepath):\n",
    "        download_if_not_present(zip_url, zip_filepath)\n",
    "        with zipfile.ZipFile(zip_filepath, \"r\") as zipf:\n",
    "            zipf.extractall(\"data/\")\n",
    "    rnaseq = pd.read_csv(rnaseq_filepath, index_col=0)\n",
    "\n",
    "    # Step 2: Load gene network, hierarchies, and driver genes\n",
    "    hierarchies = pd.read_csv(\"data/gene_to_pathway_final_with_hierarchy.csv\")\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes_2.csv\")['gene'].dropna()\n",
    "    gene_network = nx.read_edgelist(\"data/filtered_gene_network.edgelist\", nodetype=str)\n",
    "    ensembl_to_hgnc = dict(zip(hierarchies['Ensembl_ID'], hierarchies['HGNC']))\n",
    "    mapped_gene_network = nx.relabel_nodes(gene_network, ensembl_to_hgnc)\n",
    "\n",
    "    # Step 3: Filter RNA-seq data and identify valid nodes\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    valid_nodes = set(filtered_rna.columns)  # Get valid nodes after filtering RNA-seq columns\n",
    "\n",
    "    # Step 4: Create edge tensors for the graph\n",
    "    edges_df = pd.DataFrame(\n",
    "        list(mapped_gene_network.edges(data=\"weight\")),\n",
    "        columns=[\"source\", \"target\", \"weight\"]\n",
    "    )\n",
    "    edges_df[\"weight\"] = edges_df[\"weight\"].fillna(1.0).astype(float)\n",
    "    filtered_edges = edges_df[\n",
    "        (edges_df[\"source\"].isin(valid_nodes)) & (edges_df[\"target\"].isin(valid_nodes))\n",
    "    ]\n",
    "    node_to_idx = {node: idx for idx, node in enumerate(valid_nodes)}\n",
    "    filtered_edges[\"source_idx\"] = filtered_edges[\"source\"].map(node_to_idx)\n",
    "    filtered_edges[\"target_idx\"] = filtered_edges[\"target\"].map(node_to_idx)\n",
    "    edge_index = torch.tensor(filtered_edges[[\"source_idx\", \"target_idx\"]].values.T, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(filtered_edges[\"weight\"].values, dtype=torch.float32)\n",
    "\n",
    "    # Step 5: Process the hierarchy to create pathway groups\n",
    "    filtered_hierarchy = hierarchies[hierarchies[\"HGNC\"].isin(valid_nodes)]\n",
    "    pathway_dict = {\n",
    "        gene: pathway.split(':', 1)[1].split('[', 1)[0].strip() if isinstance(pathway, str) and ':' in pathway else None\n",
    "        for gene, pathway in zip(filtered_hierarchy['HGNC'], filtered_hierarchy['Level_1'])\n",
    "    }\n",
    "    grouped_pathway_dict = {}\n",
    "    for gene, pathway in pathway_dict.items():\n",
    "        if pathway:\n",
    "            grouped_pathway_dict.setdefault(pathway, []).append(gene)\n",
    "    pathway_groups = {\n",
    "        pathway: [node_to_idx[gene] for gene in genes if gene in node_to_idx]\n",
    "        for pathway, genes in grouped_pathway_dict.items()\n",
    "    }\n",
    "    # Convert to padded tensor\n",
    "    pathway_tensors = pad_sequence(\n",
    "        [torch.tensor(indices, dtype=torch.long) for indices in pathway_groups.values()], \n",
    "        batch_first=True, \n",
    "        padding_value=-1  # Use -1 as padding\n",
    "    )\n",
    "\n",
    "    # Step 6: Create cell-line graphs\n",
    "    tensor_exp = torch.tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    graph_data_list = {}\n",
    "    for cell, x in cell_dict.items():\n",
    "        if x.ndim == 2 and x.shape[0] == 1:\n",
    "            x = x.T\n",
    "        elif x.ndim == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "        graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        graph_data.y = None\n",
    "        graph_data.cell_line = cell\n",
    "        graph_data_list[cell] = graph_data\n",
    "\n",
    "    # Step 7: Load drug data\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R=fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "\n",
    "    # Step 8: Load IC50 data and filter for valid cell lines and drugs\n",
    "    data = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "\n",
    "    # Step 9: Split the data into folds for cross-validation\n",
    "    unique_cell_lines = data[\"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420)\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "\n",
    "    # Step 10: Build the datasets for training, validation, and testing\n",
    "    train_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, train_data)\n",
    "    validation_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, validation_data)\n",
    "    test_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, test_data)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset, pathway_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6194c2e-e218-4089-931e-0208470ee7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_938046/3102354241.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_edges[\"source_idx\"] = filtered_edges[\"source\"].map(node_to_idx)\n",
      "/tmp/ipykernel_938046/3102354241.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_edges[\"target_idx\"] = filtered_edges[\"target\"].map(node_to_idx)\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:38] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:39] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n",
      "[18:30:40] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_graphs, val_graphs, test_graphs, pathway_tensors = get_data(n_fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b0c2da6-a1ef-4360-a518-c26c80038c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Create a DataLoader for training graphs\n",
    "batch_size = 8  # Adjust based on memory and dataset size\n",
    "train_loader = DataLoader(train_graphs, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff46b11-d884-4eed-a329-79137fa9dc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with 'mean' string-based aggregator\n",
      "Initialized with 'mean' string-based aggregator\n",
      "Initialized with 'mean' string-based aggregator\n"
     ]
    }
   ],
   "source": [
    "# Configuration for the GNN\n",
    "config = {\n",
    "    \"gnn\": {\n",
    "        \"input_dim\": 1, \n",
    "        \"hidden_dim\": 128,\n",
    "        \"output_dim\": 1,  \n",
    "        \"pathway_groups\": pathway_tensors, \n",
    "        \"layer_modes\": [True, True, True], \n",
    "        \"pooling_mode\": \"scalar\", \n",
    "        \"aggr_modes\": [\"mean\", \"mean\", \"mean\"], \n",
    "        \"num_pathways_per_instance\": 44\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ModularGNN(\n",
    "    input_dim=config[\"gnn\"][\"input_dim\"],\n",
    "    hidden_dim=config[\"gnn\"][\"hidden_dim\"],\n",
    "    output_dim=config[\"gnn\"][\"output_dim\"],\n",
    "    pathway_groups=config[\"gnn\"][\"pathway_groups\"],\n",
    "    layer_modes=config[\"gnn\"][\"layer_modes\"],\n",
    "    aggr_modes=config[\"gnn\"][\"aggr_modes\"],\n",
    "    pooling_mode=config[\"gnn\"][\"pooling_mode\"],\n",
    "    num_pathways_per_instance=config[\"gnn\"][\"num_pathways_per_instance\"]\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7729219-7978-495e-85b6-7aaa113be38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 started...\n",
      "tensor([[ 5.1997],\n",
      "        [ 4.4034],\n",
      "        [-0.1794],\n",
      "        [ 4.3953],\n",
      "        [ 2.7380],\n",
      "        [ 4.5155],\n",
      "        [ 0.0684],\n",
      "        [ 3.8169]])\n",
      "output :tensor([[-2509815.5000],\n",
      "        [-3903638.2500],\n",
      "        [ 1257003.6250],\n",
      "        [ -256264.4844],\n",
      "        [-2954594.2500],\n",
      "        [-3594986.7500],\n",
      "        [ -875080.7500],\n",
      "        [ 1843372.5000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f15a58bc3a0>\n",
      "Epoch 1/5, Batch 1/26017, Loss: 6125092601856.0000\n",
      "tensor([[ 3.6359],\n",
      "        [ 0.7457],\n",
      "        [ 5.3806],\n",
      "        [ 3.4412],\n",
      "        [ 2.2966],\n",
      "        [-2.3218],\n",
      "        [ 2.4261],\n",
      "        [-2.2111]])\n",
      "output :tensor([[10755724.],\n",
      "        [21697042.],\n",
      "        [12543717.],\n",
      "        [21653284.],\n",
      "        [12043128.],\n",
      "        [11595840.],\n",
      "        [ 8515042.],\n",
      "        [ 9581232.]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5bf100>\n",
      "Epoch 1/5, Batch 2/26017, Loss: 207057839980544.0000\n",
      "tensor([[ 4.8435],\n",
      "        [-3.3341],\n",
      "        [ 2.7579],\n",
      "        [ 3.6463],\n",
      "        [ 0.4034],\n",
      "        [-0.1014],\n",
      "        [ 4.5472],\n",
      "        [ 2.5948]])\n",
      "output :tensor([[-5457373.5000],\n",
      "        [ 5958657.5000],\n",
      "        [ 6417230.0000],\n",
      "        [-4175159.5000],\n",
      "        [ 1614989.1250],\n",
      "        [ 6247522.5000],\n",
      "        [ 9050381.0000],\n",
      "        [-3933407.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5fecb0>\n",
      "Epoch 1/5, Batch 3/26017, Loss: 32865268006912.0000\n",
      "tensor([[ 0.7446],\n",
      "        [-0.8001],\n",
      "        [ 2.0325],\n",
      "        [ 4.9583],\n",
      "        [ 1.0031],\n",
      "        [ 4.2726],\n",
      "        [ 4.0879],\n",
      "        [ 2.9006]])\n",
      "output :tensor([[ -9265794.],\n",
      "        [ -8858008.],\n",
      "        [-12131700.],\n",
      "        [-11721902.],\n",
      "        [ -8383615.],\n",
      "        [-11614989.],\n",
      "        [-11286844.],\n",
      "        [-10744963.]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b993580>\n",
      "Epoch 1/5, Batch 4/26017, Loss: 112117604679680.0000\n",
      "tensor([[ 1.6650],\n",
      "        [ 0.0945],\n",
      "        [ 4.7633],\n",
      "        [ 3.4034],\n",
      "        [ 4.5724],\n",
      "        [-0.1510],\n",
      "        [ 1.3868],\n",
      "        [ 2.5245]])\n",
      "output :tensor([[-12924036.],\n",
      "        [-13452818.],\n",
      "        [ -9244830.],\n",
      "        [ -9169374.],\n",
      "        [ -8826382.],\n",
      "        [ -9826344.],\n",
      "        [ -9205568.],\n",
      "        [ -9635044.]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5bf100>\n",
      "Epoch 1/5, Batch 5/26017, Loss: 108699037204480.0000\n",
      "tensor([[ 3.8129],\n",
      "        [ 3.3928],\n",
      "        [-2.6519],\n",
      "        [ 4.7460],\n",
      "        [ 3.7955],\n",
      "        [ 2.1517],\n",
      "        [ 2.6136],\n",
      "        [ 5.3575]])\n",
      "output :tensor([[ -6646619.5000],\n",
      "        [ -6077790.5000],\n",
      "        [ -5628445.5000],\n",
      "        [-11705594.0000],\n",
      "        [ -5528610.5000],\n",
      "        [-12411526.0000],\n",
      "        [  -678516.2500],\n",
      "        [-15417871.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5bf100>\n",
      "Epoch 1/5, Batch 6/26017, Loss: 84075066949632.0000\n",
      "tensor([[ 1.6796],\n",
      "        [ 2.2366],\n",
      "        [ 3.8035],\n",
      "        [ 3.9194],\n",
      "        [ 3.5500],\n",
      "        [-3.9804],\n",
      "        [ 5.0502],\n",
      "        [ 5.9831]])\n",
      "output :tensor([[  7524934.5000],\n",
      "        [  2757371.0000],\n",
      "        [  5275031.5000],\n",
      "        [  5167900.5000],\n",
      "        [-12110398.0000],\n",
      "        [ -2034760.3750],\n",
      "        [  5769311.5000],\n",
      "        [  3306237.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158ba27fd0>\n",
      "Epoch 1/5, Batch 7/26017, Loss: 39222362767360.0000\n",
      "tensor([[ 5.0369],\n",
      "        [ 2.1744],\n",
      "        [ 3.2568],\n",
      "        [ 5.4028],\n",
      "        [ 5.2077],\n",
      "        [-0.3064],\n",
      "        [ 3.7786],\n",
      "        [-1.4446]])\n",
      "output :tensor([[ 5458414.0000],\n",
      "        [  357589.5625],\n",
      "        [ 8613737.0000],\n",
      "        [10150549.0000],\n",
      "        [-1705686.2500],\n",
      "        [-3378281.5000],\n",
      "        [-1213570.2500],\n",
      "        [ 7902452.5000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5bf100>\n",
      "Epoch 1/5, Batch 8/26017, Loss: 35674472316928.0000\n",
      "tensor([[ 1.4162],\n",
      "        [ 3.3549],\n",
      "        [-3.7461],\n",
      "        [ 0.9820],\n",
      "        [-0.9748],\n",
      "        [ 2.8844],\n",
      "        [ 1.1444],\n",
      "        [ 0.0073]])\n",
      "output :tensor([[ 2531311.5000],\n",
      "        [15569056.0000],\n",
      "        [15887483.0000],\n",
      "        [-2145306.2500],\n",
      "        [ 9269690.0000],\n",
      "        [  452496.8125],\n",
      "        [15886091.0000],\n",
      "        [13752617.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5bf100>\n",
      "Epoch 1/5, Batch 9/26017, Loss: 129181467803648.0000\n",
      "tensor([[ 3.5356],\n",
      "        [ 3.7634],\n",
      "        [ 1.8494],\n",
      "        [-3.5747],\n",
      "        [ 1.4996],\n",
      "        [ 2.1685],\n",
      "        [ 2.0084],\n",
      "        [ 4.1720]])\n",
      "output :tensor([[  134380.0156],\n",
      "        [ 1753149.2500],\n",
      "        [ 3409231.7500],\n",
      "        [ 5186176.0000],\n",
      "        [ 4462520.0000],\n",
      "        [ 5693141.5000],\n",
      "        [ 8652073.0000],\n",
      "        [10664168.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5bf100>\n",
      "Epoch 1/5, Batch 10/26017, Loss: 35314940772352.0000\n",
      "tensor([[ 4.0152],\n",
      "        [ 4.9318],\n",
      "        [ 0.6957],\n",
      "        [-1.0633],\n",
      "        [ 0.1124],\n",
      "        [ 2.2678],\n",
      "        [ 2.2653],\n",
      "        [ 4.0377]])\n",
      "output :tensor([[6026137.0000],\n",
      "        [6346818.0000],\n",
      "        [8593185.0000],\n",
      "        [7455749.5000],\n",
      "        [3526396.7500],\n",
      "        [7935502.0000],\n",
      "        [7196147.0000],\n",
      "        [6557503.5000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158c929f90>\n",
      "Epoch 1/5, Batch 11/26017, Loss: 47027534692352.0000\n",
      "tensor([[ 3.1852],\n",
      "        [ 5.0903],\n",
      "        [ 1.2367],\n",
      "        [-1.9279],\n",
      "        [ 4.3665],\n",
      "        [ 4.0124],\n",
      "        [ 4.6519],\n",
      "        [ 2.9448]])\n",
      "output :tensor([[3088162.7500],\n",
      "        [2373172.2500],\n",
      "        [3075633.2500],\n",
      "        [2792431.2500],\n",
      "        [2667104.5000],\n",
      "        [3019034.5000],\n",
      "        [2676771.7500],\n",
      "        [2447889.7500]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f15a58bc3a0>\n",
      "Epoch 1/5, Batch 12/26017, Loss: 7726381400064.0000\n",
      "tensor([[-0.1310],\n",
      "        [ 2.4493],\n",
      "        [-1.7422],\n",
      "        [ 1.2549],\n",
      "        [ 3.0288],\n",
      "        [ 3.3179],\n",
      "        [ 2.1728],\n",
      "        [ 4.6636]])\n",
      "output :tensor([[ -633934.5000],\n",
      "        [  519713.1250],\n",
      "        [ 1201420.3750],\n",
      "        [-2294304.5000],\n",
      "        [  267076.7812],\n",
      "        [  697854.9375],\n",
      "        [  165062.3750],\n",
      "        [  823204.9375]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158c929f90>\n",
      "Epoch 1/5, Batch 13/26017, Loss: 1080306761728.0000\n",
      "tensor([[0.2656],\n",
      "        [0.4616],\n",
      "        [0.1129],\n",
      "        [1.8431],\n",
      "        [4.4697],\n",
      "        [1.1141],\n",
      "        [4.8358],\n",
      "        [0.7081]])\n",
      "output :tensor([[-6403244.0000],\n",
      "        [-5459432.0000],\n",
      "        [-7238766.0000],\n",
      "        [-2753934.2500],\n",
      "        [-5693525.5000],\n",
      "        [ -474992.9375],\n",
      "        [-6234215.0000],\n",
      "        [-6343198.5000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158c929f90>\n",
      "Epoch 1/5, Batch 14/26017, Loss: 30316802605056.0000\n",
      "tensor([[ 3.8854],\n",
      "        [ 4.3802],\n",
      "        [ 2.2830],\n",
      "        [-1.2044],\n",
      "        [ 1.7264],\n",
      "        [ 2.6125],\n",
      "        [ 2.5347],\n",
      "        [-3.0935]])\n",
      "output :tensor([[ -9431181.0000],\n",
      "        [ -7658164.5000],\n",
      "        [ -8455290.0000],\n",
      "        [ -3134124.7500],\n",
      "        [ -2177585.0000],\n",
      "        [ -4559043.5000],\n",
      "        [ -8602446.0000],\n",
      "        [-10011251.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158c929f90>\n",
      "Epoch 1/5, Batch 15/26017, Loss: 53582934048768.0000\n",
      "tensor([[-3.4838],\n",
      "        [-2.4456],\n",
      "        [ 4.3270],\n",
      "        [ 2.8236],\n",
      "        [ 1.1996],\n",
      "        [ 2.6483],\n",
      "        [-2.7585],\n",
      "        [-2.5577]])\n",
      "output :tensor([[ -6488812.0000],\n",
      "        [ -1582877.0000],\n",
      "        [ -5367515.0000],\n",
      "        [-10943569.0000],\n",
      "        [ -9354498.0000],\n",
      "        [ -2147996.5000],\n",
      "        [ -1936903.5000],\n",
      "        [ -2113907.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158c929f90>\n",
      "Epoch 1/5, Batch 16/26017, Loss: 36690357911552.0000\n",
      "tensor([[ 2.9065],\n",
      "        [ 1.0672],\n",
      "        [ 4.6092],\n",
      "        [-6.3422],\n",
      "        [ 5.4533],\n",
      "        [ 2.3110],\n",
      "        [ 5.5866],\n",
      "        [-0.9769]])\n",
      "output :tensor([[   60580.0781],\n",
      "        [-5624471.5000],\n",
      "        [-3711904.0000],\n",
      "        [-2597308.2500],\n",
      "        [  507978.3438],\n",
      "        [-5564733.5000],\n",
      "        [-6295171.0000],\n",
      "        [-6575194.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5bf100>\n",
      "Epoch 1/5, Batch 17/26017, Loss: 20781165707264.0000\n",
      "tensor([[2.4871],\n",
      "        [1.2865],\n",
      "        [2.5632],\n",
      "        [5.3438],\n",
      "        [1.9226],\n",
      "        [2.7266],\n",
      "        [5.4460],\n",
      "        [1.4062]])\n",
      "output :tensor([[ 3717282.0000],\n",
      "        [-3076396.2500],\n",
      "        [ 5505564.0000],\n",
      "        [-5714949.5000],\n",
      "        [  272333.8438],\n",
      "        [-3640153.0000],\n",
      "        [ 6142837.0000],\n",
      "        [ 4818010.0000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b659cf0>\n",
      "Epoch 1/5, Batch 18/26017, Loss: 20065848131584.0000\n",
      "tensor([[ 1.8288],\n",
      "        [ 3.6799],\n",
      "        [ 6.0320],\n",
      "        [ 0.5021],\n",
      "        [-0.0521],\n",
      "        [ 2.2171],\n",
      "        [ 1.0037],\n",
      "        [ 3.5987]])\n",
      "output :tensor([[ 4926649.0000],\n",
      "        [-3396699.5000],\n",
      "        [-2310511.5000],\n",
      "        [11336475.0000],\n",
      "        [ 4902589.5000],\n",
      "        [-1291233.7500],\n",
      "        [ 7302759.5000],\n",
      "        [-4326242.5000]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f18141ae9b0>\n",
      "Epoch 1/5, Batch 19/26017, Loss: 33426616877056.0000\n",
      "tensor([[ 0.5660],\n",
      "        [ 5.3270],\n",
      "        [ 0.3303],\n",
      "        [ 4.1980],\n",
      "        [-2.6179],\n",
      "        [ 4.1209],\n",
      "        [ 1.1089],\n",
      "        [ 1.5875]])\n",
      "output :tensor([[ -223137.0156],\n",
      "        [ -567651.4375],\n",
      "        [ 3275555.2500],\n",
      "        [ -614285.3125],\n",
      "        [  650440.8750],\n",
      "        [-2156424.7500],\n",
      "        [ 3646480.5000],\n",
      "        [ 3120091.7500]], grad_fn=<MeanBackward1>)\n",
      "output shape:torch.Size([8, 1])\n",
      "outputs requires_grad: True, grad_fn: <MeanBackward1 object at 0x7f158b5d2950>\n",
      "Epoch 1/5, Batch 20/26017, Loss: 4947959087104.0000\n",
      "tensor([[ 1.9043],\n",
      "        [ 3.4125],\n",
      "        [ 2.3234],\n",
      "        [ 4.0379],\n",
      "        [ 0.5448],\n",
      "        [ 1.9431],\n",
      "        [ 3.9126],\n",
      "        [-0.1167]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     29\u001b[0m     x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m     30\u001b[0m     edge_index\u001b[38;5;241m=\u001b[39medge_index,\n\u001b[1;32m     31\u001b[0m     edge_attr\u001b[38;5;241m=\u001b[39medge_attr,\n\u001b[1;32m     32\u001b[0m     pathway_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m     batch\u001b[38;5;241m=\u001b[39mcell_graph\u001b[38;5;241m.\u001b[39mbatch \n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput shape:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/work/haarscheid/.conda/envs/GNN2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work/haarscheid/.conda/envs/GNN2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/model_GNN.py:137\u001b[0m, in \u001b[0;36mModularGNN.forward\u001b[0;34m(self, x, edge_index, edge_attr, pathway_tensor, batch)\u001b[0m\n\u001b[1;32m    135\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 137\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, edge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr, pathway_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_modes[i], pathway_tensor\u001b[38;5;241m=\u001b[39mpathway_groups_shifted,batch\u001b[38;5;241m=\u001b[39mbatch)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpathway\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pathway_groups_shifted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/work/haarscheid/.conda/envs/GNN2/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/work/haarscheid/.conda/envs/GNN2/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/model_GNN.py:33\u001b[0m, in \u001b[0;36mModularPathwayConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, pathway_mode, pathway_tensor, batch)\u001b[0m\n\u001b[1;32m     31\u001b[0m     x_updated \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_attr\u001b[38;5;241m=\u001b[39medge_attr) \u001b[38;5;241m+\u001b[39m x)\u001b[38;5;66;03m#/2\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     x_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_pathways(x, edge_index, edge_attr, pathway_tensor, batch)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_updated\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/model_GNN.py:51\u001b[0m, in \u001b[0;36mModularPathwayConv._process_pathways\u001b[0;34m(self, x, edge_index, edge_attr, pathway_tensors, batch)\u001b[0m\n\u001b[1;32m     49\u001b[0m sub_edge_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     edge_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misin(edge_index[\u001b[38;5;241m0\u001b[39m], nodes) \u001b[38;5;241m&\u001b[39m torch\u001b[38;5;241m.\u001b[39misin(edge_index[\u001b[38;5;241m1\u001b[39m], nodes)\n\u001b[1;32m     52\u001b[0m     sub_edge_attr \u001b[38;5;241m=\u001b[39m edge_attr[edge_mask]\n\u001b[1;32m     54\u001b[0m x_propagated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(sub_edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_attr\u001b[38;5;241m=\u001b[39msub_edge_attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} started...\")\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        try:\n",
    "            # Unpack batch components\n",
    "            cell_graph_batch, drug_tensor_batch, target_batch, cell_id_batch, drug_id_batch = batch\n",
    "        except Exception as e:\n",
    "            print(f\"Error unpacking batch {batch_idx}: {e}\")\n",
    "            print(f\"Batch contents: {batch}\")\n",
    "            continue\n",
    "        print(target_batch)\n",
    "        # Extract node features (x) and other components\n",
    "        cell_graph = cell_graph_batch.to(device)  # PyG Data object for the cell graph\n",
    "        x = cell_graph.x  # Extract node features\n",
    "        edge_index = cell_graph.edge_index  # Extract edge indices\n",
    "        edge_attr = cell_graph.edge_attr if hasattr(cell_graph, \"edge_attr\") else None  # Extract edge attributes\n",
    "        drug_vector = drug_tensor_batch.to(device)  # Tensor for drug features\n",
    "        targets = target_batch.to(device)  # Tensor for target outputs\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            pathway_tensor=None,\n",
    "            batch=cell_graph.batch \n",
    "        )\n",
    "        print(f\"output :{outputs}\")\n",
    "        print(f\"output shape:{outputs.shape}\")\n",
    "        print(f\"outputs requires_grad: {outputs.requires_grad}, grad_fn: {outputs.grad_fn}\")\n",
    "        # Loss computation\n",
    "        loss = F.mse_loss(outputs.squeeze(), targets.squeeze())  # Example: regression loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Feedback for each batch\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Feedback after each epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} completed. Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6550f3-eb95-4bf8-b081-242750829244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define node features and edge index for two graphs\n",
    "x1 = torch.tensor([[1.0], [1.0], [1.0]])  # Node features for graph 1\n",
    "edge_index1 = torch.tensor([[ 1, 2],  # Source nodes\n",
    "                            [0, 0]])  # Target nodes\n",
    "edge_attr1 = torch.tensor([ 1.0, 1.0])  # Edge attributes for graph 1\n",
    "\n",
    "x2 = torch.tensor([[5.0], [2.0], [5.0]])  # Node features for graph 2\n",
    "edge_index2 = torch.tensor([[ 1, 2],  # Source nodes\n",
    "                            [ 0, 1]])  # Target nodes\n",
    "edge_attr2 = torch.tensor([ 2.0, 1.0])  # Edge attributes for graph 2\n",
    "\n",
    "# Define pathways\n",
    "# Pathway tensor for Graph 1\n",
    "pathway_tensor1 = torch.tensor([\n",
    "    [2, 1, -1],  # Pathway 1 (nodes 0 and 1)\n",
    "    [0, -1, -1]  # Pathway 2 (node 2)\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Pathway tensor for Graph 2\n",
    "pathway_tensor2 = torch.tensor([\n",
    "    [0, 2, -1],  # Pathway 1 (nodes 0 and 2)\n",
    "    [1, -1, -1]  # Pathway 2 (node 1)\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Define node features and edge index for two graphs\n",
    "x3 = torch.tensor([[1.0], [2.0], [2.0]])  # Node features for graph 1\n",
    "edge_index3 = torch.tensor([[ 1, 2],  # Source nodes\n",
    "                            [0, 0]])  # Target nodes\n",
    "edge_attr3 = torch.tensor([ 1.0, 2.0])  # Edge attributes for graph 1\n",
    "\n",
    "x4 = torch.tensor([[5.0], [2.0], [5.0]])  # Node features for graph 2\n",
    "edge_index4 = torch.tensor([[ 1, 2],  # Source nodes\n",
    "                            [ 0, 1]])  # Target nodes\n",
    "edge_attr4 = torch.tensor([ 2.0, 1.0])  # Edge attributes for graph 2\n",
    "\n",
    "# Define pathways\n",
    "# Pathway tensor for Graph 1\n",
    "pathway_tensor3 = torch.tensor([\n",
    "    [2, 1, -1],  # Pathway 1 (nodes 0 and 1)\n",
    "    [0, -1, -1]  # Pathway 2 (node 2)\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Pathway tensor for Graph 2\n",
    "pathway_tensor4 = torch.tensor([\n",
    "    [0, 2, -1],  # Pathway 1 (nodes 0 and 2)\n",
    "    [1, -1, -1]  # Pathway 2 (node 1)\n",
    "], dtype=torch.long)\n",
    "\n",
    "# Create Data objects for each graph\n",
    "data1 = Data(x=x1, edge_index=edge_index1, edge_attr=edge_attr1,pathway_tensor = pathway_tensor1)\n",
    "data2 = Data(x=x2, edge_index=edge_index2, edge_attr=edge_attr2,pathway_tensor = pathway_tensor1)\n",
    "data3 = Data(x=x3, edge_index=edge_index3, edge_attr=edge_attr3,pathway_tensor = pathway_tensor1)\n",
    "data4 = Data(x=x4, edge_index=edge_index4, edge_attr=edge_attr4,pathway_tensor = pathway_tensor1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Batch the data\n",
    "batch = Batch.from_data_list([data1, data2,data3,data4])\n",
    "print(batch.pathway_tensor)\n",
    "\n",
    "\n",
    "# Visualize the graphs\n",
    "def visualize_graph(data, title):\n",
    "    G = to_networkx(data, edge_attrs=[\"edge_attr\"])\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500)\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{d[\"edge_attr\"]}' for u, v, d in G.edges(data=True)})\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "visualize_graph(data1, \"Graph 1\")\n",
    "visualize_graph(data2, \"Graph 2\")\n",
    "\n",
    "## Check batch\n",
    "#print(f\"Batch node features:\\n{batch.x}\")\n",
    "#print(f\"Batch edge index:\\n{batch.edge_index}\")\n",
    "#print(f\"Batch edge attributes:\\n{batch.edge_attr}\")\n",
    "#print(f\"Batch pathways:\\n{batch.pathway_tensor}\")\n",
    "#print(f\"Batch assignment:\\n{batch.batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41881a3c-66d1-4f36-b2cf-872cf87d84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract components from the batch\n",
    "x = batch.x\n",
    "edge_index = batch.edge_index\n",
    "edge_attr = batch.edge_attr\n",
    "pathway_tensor = batch.pathway_tensor  # Pathway tensor\n",
    "batch_assignment = batch.batch  # Batch assignment vector\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(\n",
    "    x=x,\n",
    "    edge_index=edge_index,\n",
    "    edge_attr=edge_attr,\n",
    "    pathway_tensor=None,\n",
    "    batch=batch_assignment\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237f444-6ede-49a0-a25f-ab97385062a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b82807-0996-4f0c-8103-187137673432",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9507e-b1fb-4c26-849b-549e4202fafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN3",
   "language": "python",
   "name": "gnn2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
