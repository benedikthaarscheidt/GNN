{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327c5070-eab2-4bad-baab-9458ab879c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1\n",
      "11.8\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 12:01:54.162951: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-16 12:01:54.172668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734346914.187005 2956354 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734346914.191202 2956354 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-16 12:01:54.207300: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.0 MB\n",
      "Max memory allocated: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"scripts\")\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "\n",
    "# Check if PyTorch is installed\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "import tensorflow as tf\n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "print(torch.version.cuda)  # Should match 12.6 or similar\n",
    "print(torch.backends.cudnn.enabled)  # Sho\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import networkx as nx\n",
    "import scripts\n",
    "from scripts import *\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna\n",
    "import models\n",
    "from optuna.integration import TensorBoardCallback\n",
    "from model_GNN import ModularPathwayConv, ModularGNN\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "from model_ResNet import CombinedModel, ResNet, DrugMLP  \n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "print(f\"Max memory allocated: {torch.cuda.max_memory_allocated() / 1e6} MB\")\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f39fab7-2585-4105-a87b-9b115dfff2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_data(n_fold=0, fp_radius=2):\n",
    "    \"\"\"Download, process, and prepare data for use in graph-based machine learning models.\"\"\"\n",
    "    import os\n",
    "    import zipfile\n",
    "    import requests\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from torch_geometric.data import Data\n",
    "    import scripts  # Assuming scripts has required functions\n",
    "\n",
    "    def download_if_not_present(url, filepath):\n",
    "        \"\"\"Download a file from a URL if it does not exist locally.\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"File not found at {filepath}. Downloading...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(\"Download completed.\")\n",
    "        else:\n",
    "            print(f\"File already exists at {filepath}.\")\n",
    "\n",
    "    # Step 1: Download and load RNA-seq data\n",
    "    zip_url = \"https://cog.sanger.ac.uk/cmp/download/rnaseq_all_20220624.zip\"\n",
    "    zip_filepath = \"data/rnaseq.zip\"\n",
    "    rnaseq_filepath = \"data/rnaseq_normcount.csv\"\n",
    "    if not os.path.exists(rnaseq_filepath):\n",
    "        download_if_not_present(zip_url, zip_filepath)\n",
    "        with zipfile.ZipFile(zip_filepath, \"r\") as zipf:\n",
    "            zipf.extractall(\"data/\")\n",
    "    rnaseq = pd.read_csv(rnaseq_filepath, index_col=0)\n",
    "\n",
    "    # Step 2: Load gene network, hierarchies, and driver genes\n",
    "    hierarchies = pd.read_csv(\"data/gene_to_pathway_final_with_hierarchy.csv\")\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes_2.csv\")['gene'].dropna()\n",
    "    gene_network = nx.read_edgelist(\"data/filtered_gene_network.edgelist\", nodetype=str)\n",
    "    ensembl_to_hgnc = dict(zip(hierarchies['Ensembl_ID'], hierarchies['HGNC']))\n",
    "    mapped_gene_network = nx.relabel_nodes(gene_network, ensembl_to_hgnc)\n",
    "\n",
    "    # Step 3: Filter RNA-seq data and identify valid nodes\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    valid_nodes = set(filtered_rna.columns)  # Get valid nodes after filtering RNA-seq columns\n",
    "\n",
    "    # Step 4: Create edge tensors for the graph\n",
    "    edges_df = pd.DataFrame(\n",
    "        list(mapped_gene_network.edges(data=\"weight\")),\n",
    "        columns=[\"source\", \"target\", \"weight\"]\n",
    "    )\n",
    "    edges_df[\"weight\"] = edges_df[\"weight\"].fillna(1.0).astype(float)\n",
    "    filtered_edges = edges_df[\n",
    "        (edges_df[\"source\"].isin(valid_nodes)) & (edges_df[\"target\"].isin(valid_nodes))\n",
    "    ]\n",
    "    node_to_idx = {node: idx for idx, node in enumerate(valid_nodes)}\n",
    "    filtered_edges[\"source_idx\"] = filtered_edges[\"source\"].map(node_to_idx)\n",
    "    filtered_edges[\"target_idx\"] = filtered_edges[\"target\"].map(node_to_idx)\n",
    "    edge_index = torch.tensor(filtered_edges[[\"source_idx\", \"target_idx\"]].values.T, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(filtered_edges[\"weight\"].values, dtype=torch.float32)\n",
    "\n",
    "    # Step 5: Process the hierarchy to create pathway groups\n",
    "    filtered_hierarchy = hierarchies[hierarchies[\"HGNC\"].isin(valid_nodes)]\n",
    "    pathway_dict = {\n",
    "        gene: pathway.split(':', 1)[1].split('[', 1)[0].strip() if isinstance(pathway, str) and ':' in pathway else None\n",
    "        for gene, pathway in zip(filtered_hierarchy['HGNC'], filtered_hierarchy['Level_1'])\n",
    "    }\n",
    "    grouped_pathway_dict = {}\n",
    "    for gene, pathway in pathway_dict.items():\n",
    "        if pathway:\n",
    "            grouped_pathway_dict.setdefault(pathway, []).append(gene)\n",
    "    pathway_groups = {\n",
    "        pathway: [node_to_idx[gene] for gene in genes if gene in node_to_idx]\n",
    "        for pathway, genes in grouped_pathway_dict.items()\n",
    "    }\n",
    "    # Convert to padded tensor\n",
    "    pathway_tensors = pad_sequence(\n",
    "        [torch.tensor(indices, dtype=torch.long) for indices in pathway_groups.values()], \n",
    "        batch_first=True, \n",
    "        padding_value=-1  # Use -1 as padding\n",
    "    )\n",
    "\n",
    "    # Step 6: Create cell-line graphs\n",
    "    tensor_exp = torch.tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    graph_data_list = {}\n",
    "    for cell, x in cell_dict.items():\n",
    "        if x.ndim == 2 and x.shape[0] == 1:\n",
    "            x = x.T\n",
    "        elif x.ndim == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "        graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        graph_data.y = None\n",
    "        graph_data.cell_line = cell\n",
    "        graph_data_list[cell] = graph_data\n",
    "\n",
    "    # Step 7: Load drug data\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R=fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "\n",
    "    # Step 8: Load IC50 data and filter for valid cell lines and drugs\n",
    "    data = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "\n",
    "    # Step 9: Split the data into folds for cross-validation\n",
    "    unique_cell_lines = data[\"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420)\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "\n",
    "    # Step 10: Build the datasets for training, validation, and testing\n",
    "    train_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, train_data)\n",
    "    validation_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, validation_data)\n",
    "    test_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, test_data)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset, pathway_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93db7a6f-24d4-48e7-8dc9-9ae4b4d0448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3253249/3102354241.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_edges[\"source_idx\"] = filtered_edges[\"source\"].map(node_to_idx)\n",
      "/tmp/ipykernel_3253249/3102354241.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_edges[\"target_idx\"] = filtered_edges[\"target\"].map(node_to_idx)\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:14] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n",
      "[13:43:15] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, pathway_groups=get_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3d075f-7678-439b-9e50-aa9ba42d682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    try:\n",
    "        cell_graphs = [item[0] for item in batch if item[0] is not None]\n",
    "        if len(cell_graphs) == 0:\n",
    "            raise ValueError(\"No graphs to batch in this batch. Batch might be empty or contains None.\")\n",
    "        \n",
    "        drug_vectors = torch.stack([item[1] for item in batch if item[1] is not None])  # Stack drug vectors\n",
    "        targets = torch.stack([item[2] for item in batch if item[2] is not None])  # Stack target values\n",
    "        cell_ids = torch.stack([item[3] for item in batch if item[3] is not None])  # Stack cell IDs\n",
    "        drug_ids = torch.stack([item[4] for item in batch if item[4] is not None])  # Stack drug IDs\n",
    "\n",
    "        # Batch the PyG graphs into a single DataBatch\n",
    "        cell_graph_batch = Batch.from_data_list(cell_graphs)\n",
    "        return cell_graph_batch, drug_vectors, targets, cell_ids, drug_ids\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom_collate_fn: {e}\")\n",
    "        print(f\"Batch contents: {batch}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecbf01f-4c3b-42fb-8be4-a754fb83c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn_model = ModularGNN(**config[\"gnn\"])\n",
    "#drug_mlp = DrugMLP(input_dim=config[\"drug\"][\"input_dim\"], embed_dim=config[\"drug\"][\"embed_dim\"])\n",
    "#resnet = ResNet(embed_dim=config[\"drug\"][\"embed_dim\"], hidden_dim=config[\"resnet\"][\"hidden_dim\"], \n",
    "#                n_layers=config[\"resnet\"][\"n_layers\"], dropout=config[\"resnet\"][\"dropout\"])\n",
    "#\n",
    "#combined_model = CombinedModel(gnn=gnn_model, drug_mlp=drug_mlp, resnet=resnet)\n",
    "#\n",
    "#device = torch.device(config[\"env\"][\"device\"])\n",
    "#combined_model.to(device)\n",
    "#print(pathway_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e27089-9ed4-45fa-84c0-30782a46f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the global configuration\n",
    "config = {\n",
    "    \"gnn\": {\n",
    "        \"input_dim\": 1,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"output_dim\": 128,\n",
    "        \"pathway_groups\": None,  # Specify pathway groups if required\n",
    "        \"layer_modes\": [True, True, True],\n",
    "        \"pooling_mode\": \"pathway\",\n",
    "        \"aggr_modes\": [\"mean\", \"mean\", \"mean\"],\n",
    "        \"num_pathways_per_instance\": 44\n",
    "    },\n",
    "    \"resnet\": {\n",
    "        \"hidden_dim\": 512,\n",
    "        \"n_layers\": 6,\n",
    "        \"dropout\": 0.1,\n",
    "    },\n",
    "    \"drug\": {\n",
    "        \"input_dim\": 2048,\n",
    "        \"embed_dim\": 128\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 2,\n",
    "        \"clip_norm\": 1.0,\n",
    "        \"stopping_patience\": 10,\n",
    "    },\n",
    "    \"env\": {\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"max_epochs\": 50,\n",
    "        \"search_hyperparameters\": True  \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578d616-6f7b-4936-9516-d526e86f32e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-15 13:43:17,569] A new study created in RDB with name: baseline_model_885e7cd3\n",
      "/work/haarscheid/.conda/envs/GNN2/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ['sum', 'mean', 'max'] which is of type list.\n",
      "  warnings.warn(message)\n",
      "/work/haarscheid/.conda/envs/GNN2/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ['mean', 'mean', 'mean'] which is of type list.\n",
      "  warnings.warn(message)\n",
      "/work/haarscheid/.conda/envs/GNN2/lib/python3.12/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains ['max', 'max', 'max'] which is of type list.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " === Starting Trial #0 === \n",
      "\n",
      " **Trial Hyperparameters**:\n",
      "\n",
      " **GNN**\n",
      "  - input_dim: 1\n",
      "  - hidden_dim: 426\n",
      "  - output_dim: 115\n",
      "  - pathway_groups: None\n",
      "  - layer_modes: [True, True, True]\n",
      "  - pooling_mode: None\n",
      "  - aggr_modes: ['sum', 'mean', 'max']\n",
      "  - num_pathways_per_instance: 44\n",
      "\n",
      " **RESNET**\n",
      "  - hidden_dim: 80\n",
      "  - n_layers: 2\n",
      "  - dropout: 0.4300395745307493\n",
      "\n",
      " **DRUG**\n",
      "  - input_dim: 2048\n",
      "  - embed_dim: 128\n",
      "\n",
      " **OPTIMIZER**\n",
      "  - learning_rate: 0.001209918724302865\n",
      "  - batch_size: 6\n",
      "  - clip_norm: 1.0\n",
      "  - stopping_patience: 10\n",
      "\n",
      " **ENV**\n",
      "  - device: cpu\n",
      "  - max_epochs: 50\n",
      "  - search_hyperparameters: True\n",
      "\n",
      "===============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0:   0%|                                                                                                                                                                                                                                           | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with 'sum' string-based aggregator\n",
      "Initialized with 'mean' string-based aggregator\n",
      "Initialized with 'max' string-based aggregator\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "Lazy layers initialized successfully with a real batch instance.\n",
      "training step\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n",
      "cell embedding shape:torch.Size([6, 115])\n",
      "drug_embedding shape:torch.Size([6, 115])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm  # Import tqdm for the progress bars\n",
    "\n",
    "def train_model_optuna(trial, base_config, train_dataset, validation_dataset):\n",
    "    \"\"\"Optuna wrapper that defines hyperparameters and calls train_model.\"\"\"\n",
    "    copy_config = deepcopy(base_config)\n",
    "\n",
    "    def pruning_callback(epoch, train_r):\n",
    "        trial.report(train_r, step=epoch)\n",
    "        if np.isnan(train_r):\n",
    "            raise optuna.TrialPruned()\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    # Determine available pooling modes\n",
    "    available_pooling_modes = [\"scalar\", None]  # Default possible pooling modes\n",
    "    if copy_config[\"gnn\"].get(\"pathway_groups\") is not None:\n",
    "        available_pooling_modes.append(\"pathway\")  # Only add \"pathway\" if pathway groups are defined\n",
    "\n",
    "    # Dynamic hyperparameters via Optuna\n",
    "    copy_config[\"gnn\"][\"hidden_dim\"] = trial.suggest_int(\"gnn_hidden_dim\", 64, 512)\n",
    "    copy_config[\"gnn\"][\"output_dim\"] = trial.suggest_int(\"output_dim\", 64, 256)\n",
    "    copy_config[\"gnn\"][\"pooling_mode\"] = trial.suggest_categorical(\"pooling_mode\", available_pooling_modes)\n",
    "    copy_config[\"gnn\"][\"aggr_modes\"] = trial.suggest_categorical(\"aggr_modes\", [[\"sum\", \"mean\", \"max\"], [\"mean\", \"mean\", \"mean\"], [\"max\", \"max\", \"max\"]])\n",
    "\n",
    "    copy_config[\"optimizer\"][\"learning_rate\"] = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n",
    "    copy_config[\"optimizer\"][\"batch_size\"] = trial.suggest_int(\"batch_size\", 1, 8)\n",
    "    \n",
    "    copy_config[\"resnet\"][\"hidden_dim\"] = trial.suggest_int(\"hidden_dim_resnet\", 64, 512)\n",
    "    copy_config[\"resnet\"][\"n_layers\"] = trial.suggest_int(\"n_layers\", 1, 6)\n",
    "    copy_config[\"resnet\"][\"dropout\"] = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    \n",
    "    device = torch.device(copy_config[\"env\"][\"device\"])\n",
    "\n",
    "    print(\"\\n\\n === Starting Trial #{} === \".format(trial.number))\n",
    "    print(\"\\n **Trial Hyperparameters**:\")\n",
    "    for section, params in copy_config.items():\n",
    "        print(f\"\\n **{section.upper()}**\")\n",
    "        for key, value in params.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "    print(\"\\n===============================\\n\")\n",
    "\n",
    "    if copy_config[\"gnn\"][\"pooling_mode\"] == 'pathway' and copy_config[\"gnn\"].get(\"pathway_groups\") is None:\n",
    "        print(f\" Error: Pathway pooling is not allowed without pathway groups.\")\n",
    "        raise ValueError(\"Pathway pooling requires pathway groups to be defined in the configuration.\")\n",
    "\n",
    "    try:\n",
    "        # Use tqdm to create a status bar for the trial\n",
    "        with tqdm(total=copy_config['env']['max_epochs'], desc=f\"Trial {trial.number}\", position=0, leave=True) as epoch_bar:\n",
    "            # Call train_model and get the result (R) for Optuna\n",
    "            R, _ = scripts.train_model(\n",
    "                copy_config, \n",
    "                train_dataset, \n",
    "                validation_dataset, \n",
    "                callback_epoch=lambda epoch, r: epoch_bar.update(1)  # Update tqdm bar per epoch\n",
    "            )\n",
    "            return R\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred during Optuna trial: {e}\")\n",
    "        return 0\n",
    "\n",
    "if config[\"env\"][\"search_hyperparameters\"]:\n",
    "    study_name = \"baseline_model\"\n",
    "    storage_name = f\"sqlite:///studies/{study_name}.db\"\n",
    "    unique_study_name = f\"baseline_model_{str(uuid.uuid4())[:8]}\"  # Random suffix for uniqueness\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        study_name=unique_study_name,\n",
    "        storage=storage_name,\n",
    "        direction='maximize',\n",
    "        load_if_exists=False,  # No issue since the study name is always unique\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=30, n_warmup_steps=5, interval_steps=5)\n",
    "    )\n",
    "\n",
    "    study.optimize(lambda trial: train_model_optuna(trial, config, train_data, val_data), n_trials=40)\n",
    "\n",
    "    best_config = study.best_params\n",
    "    print(\"\\n\\n=== Best Configuration from Optuna ===\")\n",
    "    print(best_config)\n",
    "\n",
    "    # Update the configuration with the best trial values\n",
    "    config[\"gnn\"].update({\n",
    "        \"hidden_dim\": best_config[\"gnn_hidden_dim\"],\n",
    "        \"output_dim\": best_config[\"output_dim\"],\n",
    "        \"pooling_mode\": best_config[\"pooling_mode\"],\n",
    "        \"aggr_modes\": best_config[\"aggr_modes\"]\n",
    "    })\n",
    "    \n",
    "    config[\"resnet\"].update({\n",
    "        \"hidden_dim\": best_config[\"hidden_dim_resnet\"],\n",
    "        \"n_layers\": best_config[\"n_layers\"],\n",
    "        \"dropout\": best_config[\"dropout\"]\n",
    "    })\n",
    "    \n",
    "    config[\"optimizer\"].update({\n",
    "        \"learning_rate\": best_config[\"learning_rate\"],\n",
    "        \"batch_size\": best_config[\"batch_size\"]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n\\n=== Final Training with Best Configuration ===\")\n",
    "    _, model = scripts.train_model(config, train_data, val_data, use_momentum=False)\n",
    "    \n",
    "    device = torch.device(config[\"env\"][\"device\"])\n",
    "    metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection({\n",
    "        \"R_cellwise_residuals\": torchmetrics.PearsonCorrCoef(num_outputs=1),\n",
    "        \"R_cellwise\": torchmetrics.PearsonCorrCoef(num_outputs=1),\n",
    "        \"MSE\": torchmetrics.MeanSquaredError()\n",
    "    }))\n",
    "    metrics.to(device)\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_data, \n",
    "        batch_size=config[\"optimizer\"][\"batch_size\"],\n",
    "        shuffle=True, \n",
    "        drop_last=True, \n",
    "        collate_fn=custom_collate_fn  \n",
    "    )\n",
    "\n",
    "    final_metrics = evaluate_step(model, test_dataloader, metrics, device)\n",
    "    print(\"\\n\\n=== Final Test Metrics ===\")\n",
    "    print(final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfe888-453c-406d-94b6-634d8e5facd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN4",
   "language": "python",
   "name": "gnn3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
