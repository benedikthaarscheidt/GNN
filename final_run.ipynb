{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "327c5070-eab2-4bad-baab-9458ab879c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 17:45:53.086533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734367553.100591 3118152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734367553.104965 3118152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-16 17:45:53.121319: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 2\n",
      "CUDA Available: True\n",
      "CUDA Version: 12.4\n",
      "Memory allocated: 0.0 MB\n",
      "Max memory allocated: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r\"scripts\")\n",
    "from functools import lru_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "\n",
    "# Check if PyTorch is installed\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "import tensorflow as tf\n",
    "print(f\"Num GPUs Available: {len(tf.config.experimental.list_physical_devices('GPU'))}\")\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import networkx as nx\n",
    "import scripts\n",
    "from scripts import *\n",
    "import torchmetrics\n",
    "from torch import nn\n",
    "import optuna\n",
    "import models\n",
    "from optuna.integration import TensorBoardCallback\n",
    "from model_GNN import ModularPathwayConv, ModularGNN\n",
    "torch.set_printoptions(threshold=torch.inf)\n",
    "from model_ResNet import CombinedModel, ResNet, DrugMLP  \n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from copy import deepcopy\n",
    "import itertools\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1e6} MB\")\n",
    "print(f\"Max memory allocated: {torch.cuda.max_memory_allocated() / 1e6} MB\")\n",
    "import uuid\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import  DistributedSampler\n",
    "from tqdm import tqdm  # Import tqdm for the progress bars\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f39fab7-2585-4105-a87b-9b115dfff2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def get_data(n_fold=0, fp_radius=2):\n",
    "    \"\"\"Download, process, and prepare data for use in graph-based machine learning models.\"\"\"\n",
    "    import os\n",
    "    import zipfile\n",
    "    import requests\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from torch_geometric.data import Data\n",
    "    import scripts  # Assuming scripts has required functions\n",
    "\n",
    "    def download_if_not_present(url, filepath):\n",
    "        \"\"\"Download a file from a URL if it does not exist locally.\"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"File not found at {filepath}. Downloading...\")\n",
    "            response = requests.get(url, stream=True)\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    file.write(chunk)\n",
    "            print(\"Download completed.\")\n",
    "        else:\n",
    "            print(f\"File already exists at {filepath}.\")\n",
    "\n",
    "    # Step 1: Download and load RNA-seq data\n",
    "    zip_url = \"https://cog.sanger.ac.uk/cmp/download/rnaseq_all_20220624.zip\"\n",
    "    zip_filepath = \"data/rnaseq.zip\"\n",
    "    rnaseq_filepath = \"data/rnaseq_normcount.csv\"\n",
    "    if not os.path.exists(rnaseq_filepath):\n",
    "        download_if_not_present(zip_url, zip_filepath)\n",
    "        with zipfile.ZipFile(zip_filepath, \"r\") as zipf:\n",
    "            zipf.extractall(\"data/\")\n",
    "    rnaseq = pd.read_csv(rnaseq_filepath, index_col=0)\n",
    "\n",
    "    # Step 2: Load gene network, hierarchies, and driver genes\n",
    "    hierarchies = pd.read_csv(\"data/gene_to_pathway_final_with_hierarchy.csv\")\n",
    "    driver_genes = pd.read_csv(\"data/driver_genes_2.csv\")['gene'].dropna()\n",
    "    gene_network = nx.read_edgelist(\"data/filtered_gene_network.edgelist\", nodetype=str)\n",
    "    ensembl_to_hgnc = dict(zip(hierarchies['Ensembl_ID'], hierarchies['HGNC']))\n",
    "    mapped_gene_network = nx.relabel_nodes(gene_network, ensembl_to_hgnc)\n",
    "\n",
    "    # Step 3: Filter RNA-seq data and identify valid nodes\n",
    "    driver_columns = rnaseq.columns.isin(driver_genes)\n",
    "    filtered_rna = rnaseq.loc[:, driver_columns]\n",
    "    valid_nodes = set(filtered_rna.columns)  # Get valid nodes after filtering RNA-seq columns\n",
    "\n",
    "    # Step 4: Create edge tensors for the graph\n",
    "    edges_df = pd.DataFrame(\n",
    "        list(mapped_gene_network.edges(data=\"weight\")),\n",
    "        columns=[\"source\", \"target\", \"weight\"]\n",
    "    )\n",
    "    edges_df[\"weight\"] = edges_df[\"weight\"].fillna(1.0).astype(float)\n",
    "    filtered_edges = edges_df[\n",
    "        (edges_df[\"source\"].isin(valid_nodes)) & (edges_df[\"target\"].isin(valid_nodes))\n",
    "    ]\n",
    "    node_to_idx = {node: idx for idx, node in enumerate(valid_nodes)}\n",
    "    filtered_edges[\"source_idx\"] = filtered_edges[\"source\"].map(node_to_idx)\n",
    "    filtered_edges[\"target_idx\"] = filtered_edges[\"target\"].map(node_to_idx)\n",
    "    edge_index = torch.tensor(filtered_edges[[\"source_idx\", \"target_idx\"]].values.T, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(filtered_edges[\"weight\"].values, dtype=torch.float32)\n",
    "\n",
    "    # Step 5: Process the hierarchy to create pathway groups\n",
    "    filtered_hierarchy = hierarchies[hierarchies[\"HGNC\"].isin(valid_nodes)]\n",
    "    pathway_dict = {\n",
    "        gene: pathway.split(':', 1)[1].split('[', 1)[0].strip() if isinstance(pathway, str) and ':' in pathway else None\n",
    "        for gene, pathway in zip(filtered_hierarchy['HGNC'], filtered_hierarchy['Level_1'])\n",
    "    }\n",
    "    grouped_pathway_dict = {}\n",
    "    for gene, pathway in pathway_dict.items():\n",
    "        if pathway:\n",
    "            grouped_pathway_dict.setdefault(pathway, []).append(gene)\n",
    "    pathway_groups = {\n",
    "        pathway: [node_to_idx[gene] for gene in genes if gene in node_to_idx]\n",
    "        for pathway, genes in grouped_pathway_dict.items()\n",
    "    }\n",
    "    # Convert to padded tensor\n",
    "    pathway_tensors = pad_sequence(\n",
    "        [torch.tensor(indices, dtype=torch.long) for indices in pathway_groups.values()], \n",
    "        batch_first=True, \n",
    "        padding_value=-1  # Use -1 as padding\n",
    "    )\n",
    "\n",
    "    # Step 6: Create cell-line graphs\n",
    "    tensor_exp = torch.tensor(filtered_rna.to_numpy())\n",
    "    cell_dict = {cell: tensor_exp[i] for i, cell in enumerate(filtered_rna.index.to_numpy())}\n",
    "    graph_data_list = {}\n",
    "    for cell, x in cell_dict.items():\n",
    "        if x.ndim == 2 and x.shape[0] == 1:\n",
    "            x = x.T\n",
    "        elif x.ndim == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "        graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        graph_data.y = None\n",
    "        graph_data.cell_line = cell\n",
    "        graph_data_list[cell] = graph_data\n",
    "\n",
    "    # Step 7: Load drug data\n",
    "    smile_dict = pd.read_csv(\"data/smiles.csv\", index_col=0)\n",
    "    fp = scripts.FingerprintFeaturizer(R=fp_radius)\n",
    "    drug_dict = fp(smile_dict.iloc[:, 1], smile_dict.iloc[:, 0])\n",
    "\n",
    "    # Step 8: Load IC50 data and filter for valid cell lines and drugs\n",
    "    data = pd.read_csv(\"data/GDSC1.csv\", index_col=0)\n",
    "    data = data.query(\"SANGER_MODEL_ID in @cell_dict.keys() & DRUG_ID in @drug_dict.keys()\")\n",
    "\n",
    "    # Step 9: Split the data into folds for cross-validation\n",
    "    unique_cell_lines = data[\"SANGER_MODEL_ID\"].unique()\n",
    "    np.random.seed(420)\n",
    "    np.random.shuffle(unique_cell_lines)\n",
    "    folds = np.array_split(unique_cell_lines, 10)\n",
    "    train_idxs = list(range(10))\n",
    "    train_idxs.remove(n_fold)\n",
    "    validation_idx = np.random.choice(train_idxs)\n",
    "    train_idxs.remove(validation_idx)\n",
    "    train_lines = np.concatenate([folds[idx] for idx in train_idxs])\n",
    "    validation_lines = folds[validation_idx]\n",
    "    test_lines = folds[n_fold]\n",
    "\n",
    "    train_data = data.query(\"SANGER_MODEL_ID in @train_lines\")\n",
    "    validation_data = data.query(\"SANGER_MODEL_ID in @validation_lines\")\n",
    "    test_data = data.query(\"SANGER_MODEL_ID in @test_lines\")\n",
    "\n",
    "    # Step 10: Build the datasets for training, validation, and testing\n",
    "    train_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, train_data)\n",
    "    validation_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, validation_data)\n",
    "    test_dataset = scripts.OmicsDataset(graph_data_list, drug_dict, test_data)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset, pathway_tensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93db7a6f-24d4-48e7-8dc9-9ae4b4d0448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3118152/3102354241.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_edges[\"source_idx\"] = filtered_edges[\"source\"].map(node_to_idx)\n",
      "/tmp/ipykernel_3118152/3102354241.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_edges[\"target_idx\"] = filtered_edges[\"target\"].map(node_to_idx)\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:31] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n",
      "[17:46:32] DEPRECATION WARNING: please use MorganGenerator\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data, pathway_groups=get_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea3d075f-7678-439b-9e50-aa9ba42d682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    try:\n",
    "        cell_graphs = [item[0] for item in batch if item[0] is not None]\n",
    "        if len(cell_graphs) == 0:\n",
    "            raise ValueError(\"No graphs to batch in this batch. Batch might be empty or contains None.\")\n",
    "        \n",
    "        drug_vectors = torch.stack([item[1] for item in batch if item[1] is not None])  # Stack drug vectors\n",
    "        targets = torch.stack([item[2] for item in batch if item[2] is not None])  # Stack target values\n",
    "        cell_ids = torch.stack([item[3] for item in batch if item[3] is not None])  # Stack cell IDs\n",
    "        drug_ids = torch.stack([item[4] for item in batch if item[4] is not None])  # Stack drug IDs\n",
    "\n",
    "        # Batch the PyG graphs into a single DataBatch\n",
    "        cell_graph_batch = Batch.from_data_list(cell_graphs)\n",
    "        return cell_graph_batch, drug_vectors, targets, cell_ids, drug_ids\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom_collate_fn: {e}\")\n",
    "        print(f\"Batch contents: {batch}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecbf01f-4c3b-42fb-8be4-a754fb83c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gnn_model = ModularGNN(**config[\"gnn\"])\n",
    "#drug_mlp = DrugMLP(input_dim=config[\"drug\"][\"input_dim\"], embed_dim=config[\"drug\"][\"embed_dim\"])\n",
    "#resnet = ResNet(embed_dim=config[\"drug\"][\"embed_dim\"], hidden_dim=config[\"resnet\"][\"hidden_dim\"], \n",
    "#                n_layers=config[\"resnet\"][\"n_layers\"], dropout=config[\"resnet\"][\"dropout\"])\n",
    "#\n",
    "#combined_model = CombinedModel(gnn=gnn_model, drug_mlp=drug_mlp, resnet=resnet)\n",
    "#\n",
    "#device = torch.device(config[\"env\"][\"device\"])\n",
    "#combined_model.to(device)\n",
    "#print(pathway_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94e27089-9ed4-45fa-84c0-30782a46f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"gnn\": {\n",
    "        \"input_dim\": 1,\n",
    "        \"hidden_dim\": 128,\n",
    "        \"output_dim\": 1,\n",
    "        \"pathway_groups\": pathway_groups,  # Specify pathway groups if required\n",
    "        \"layer_modes\": [True, True, True],\n",
    "        \"pooling_mode\": \"pathway\",\n",
    "        \"aggr_modes\": [\"mean\", \"mean\", \"mean\"],\n",
    "        \"num_pathways_per_instance\": 44\n",
    "    },\n",
    "    \"resnet\": {\n",
    "        \"hidden_dim\": 44,\n",
    "        \"n_layers\": 6,\n",
    "        \"dropout\": 0.1,\n",
    "    },\n",
    "    \"drug\": {\n",
    "        \"input_dim\": 2048,\n",
    "        \"embed_dim\": 44\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 16,\n",
    "        \"clip_norm\": 1.0,\n",
    "        \"stopping_patience\": 10,\n",
    "    },\n",
    "    \"env\": {\n",
    "        \"device\": f\"cuda:{0}\" if torch.cuda.is_available() else \"cpu\",  # Use GPU 0\n",
    "        \"max_epochs\": 50,\n",
    "        \"world_size\": 1,  # Only one GPU\n",
    "        \"rank\": 0,  # Only one process\n",
    "        \"local_rank\": 0,  # Only one GPU, so local rank is 0\n",
    "        \"master_addr\": \"localhost\",  # Master node address\n",
    "        \"master_port\": \"12345\",  # Communication port\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4259e375-639d-4034-9a57-e621a078c1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of GPUs:2\n",
      "Lazy layers initialized successfully with a real batch instance.\n",
      "Running on device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haarscheid/.conda/envs/GNN3/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: FutureWarning: The default value for `maximize` will be changed from `True` to `None` in v1.7.0 of TorchMetrics,will automatically infer the value based on the `higher_is_better` attribute of the metric (if such attribute exists) or raise an error if it does not. If you are explicitly setting the `maximize` argument to either `True` or `False` already, you can ignore this warning.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1, Loss: 107946442752.0000\n",
      "Batch 2, Loss: 85221687296.0000\n",
      "Batch 3, Loss: 41068732416.0000\n",
      "Batch 4, Loss: 13269834752.0000\n",
      "Batch 5, Loss: 18368745472.0000\n",
      "Batch 6, Loss: 4297502208.0000\n",
      "Batch 7, Loss: 3815027968.0000\n",
      "Batch 8, Loss: 6247688192.0000\n",
      "Batch 9, Loss: 9071473664.0000\n",
      "Batch 10, Loss: 4265682432.0000\n",
      "Batch 11, Loss: 76128144.0000\n",
      "Batch 12, Loss: 5389819904.0000\n",
      "Batch 13, Loss: 9044699136.0000\n",
      "Batch 14, Loss: 3945611264.0000\n",
      "Batch 15, Loss: 2737492992.0000\n",
      "Batch 16, Loss: 439898368.0000\n",
      "Batch 17, Loss: 2187254784.0000\n",
      "Batch 18, Loss: 3293343744.0000\n",
      "Batch 19, Loss: 1048024192.0000\n",
      "Batch 20, Loss: 86713744.0000\n",
      "Batch 21, Loss: 637604224.0000\n",
      "Batch 22, Loss: 173974464.0000\n",
      "Batch 23, Loss: 426823424.0000\n",
      "Batch 24, Loss: 819325440.0000\n",
      "Batch 25, Loss: 808972544.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m R, model \u001b[38;5;241m=\u001b[39m \u001b[43mscripts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mMetricTracker(torchmetrics\u001b[38;5;241m.\u001b[39mMetricCollection({ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR_cellwise_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m: torchmetrics\u001b[38;5;241m.\u001b[39mPearsonCorrCoef(num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR_cellwise\u001b[39m\u001b[38;5;124m\"\u001b[39m: torchmetrics\u001b[38;5;241m.\u001b[39mPearsonCorrCoef(num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m: torchmetrics\u001b[38;5;241m.\u001b[39mMeanSquaredError() }))\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/train_model.py:199\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(config, train_dataset, validation_dataset, callback_epoch)\u001b[0m\n\u001b[1;32m    197\u001b[0m best_val_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m--> 199\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_loader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/train_model.py:89\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, loader, config, device)\u001b[0m\n\u001b[1;32m     85\u001b[0m targets \u001b[38;5;241m=\u001b[39m target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     87\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 89\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs\u001b[38;5;241m.\u001b[39msqueeze(), targets\u001b[38;5;241m.\u001b[39msqueeze())\n\u001b[1;32m     92\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1643\u001b[0m, in \u001b[0;36mDistributedDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistributedDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1639\u001b[0m     inputs, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_forward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1640\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1641\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_delay_all_reduce_all_params\n\u001b[0;32m-> 1643\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_ddp_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1644\u001b[0m     )\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_forward(output)\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/parallel/distributed.py:1459\u001b[0m, in \u001b[0;36mDistributedDataParallel._run_ddp_forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inside_ddp_forward():\n\u001b[0;32m-> 1459\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/model_ResNet.py:123\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, cell_graph, drug_vector, pathway_tensor)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, cell_graph, drug_vector, pathway_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    121\u001b[0m \n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# Cell embedding via GNN\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     cell_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43medge_attr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcell_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpathway_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpathway_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpathway_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcell_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Handle extra dimensions in the GNN output\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cell_embedding\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cell_embedding\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m: \n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/model_GNN.py:121\u001b[0m, in \u001b[0;36mModularGNN.forward\u001b[0;34m(self, x, edge_index, edge_attr, pathway_tensor, batch)\u001b[0m\n\u001b[1;32m    118\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# No changes here\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 121\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpathway_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_modes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpathway_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpathway_groups_shifted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpathway\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pathway_groups_shifted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/GNN3/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/model_GNN.py:26\u001b[0m, in \u001b[0;36mModularPathwayConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, pathway_mode, pathway_tensor, batch)\u001b[0m\n\u001b[1;32m     24\u001b[0m     x_updated \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_attr\u001b[38;5;241m=\u001b[39medge_attr) \u001b[38;5;241m+\u001b[39m x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     x_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_pathways\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpathway_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_updated\n",
      "File \u001b[0;32m/work/haarscheid/cancer_baseline2/cancer_baseline/Graphs/scripts/model_GNN.py:45\u001b[0m, in \u001b[0;36mModularPathwayConv._process_pathways\u001b[0;34m(self, x, edge_index, edge_attr, pathway_tensors, batch)\u001b[0m\n\u001b[1;32m     42\u001b[0m         edge_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39misin(edge_index[\u001b[38;5;241m0\u001b[39m], nodes) \u001b[38;5;241m&\u001b[39m torch\u001b[38;5;241m.\u001b[39misin(edge_index[\u001b[38;5;241m1\u001b[39m], nodes)\n\u001b[1;32m     43\u001b[0m         sub_edge_attr \u001b[38;5;241m=\u001b[39m edge_attr[edge_mask]\n\u001b[0;32m---> 45\u001b[0m     x_propagated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub_edge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_edge_attr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     x_updated[nodes] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (x_propagated[nodes] \u001b[38;5;241m+\u001b[39m x[nodes])\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_updated\n",
      "File \u001b[0;32m/tmp/model_GNN_ModularPathwayConv_propagate_yvfievx_.py:119\u001b[0m, in \u001b[0;36mpropagate\u001b[0;34m(self, edge_index, x, edge_attr, size)\u001b[0m\n\u001b[1;32m    108\u001b[0m     dim_size \u001b[38;5;241m=\u001b[39m size_i\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m CollectArgs(\n\u001b[1;32m    111\u001b[0m         x_j,\n\u001b[1;32m    112\u001b[0m         edge_attr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m         dim_size,\n\u001b[1;32m    116\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpropagate\u001b[39m(\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    121\u001b[0m     edge_index: Union[Tensor, SparseTensor],\n\u001b[1;32m    122\u001b[0m     x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    123\u001b[0m     edge_attr: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    124\u001b[0m     size: Size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# Begin Propagate Forward Pre Hook #########################################\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_propagate_forward_pre_hooks\u001b[38;5;241m.\u001b[39mvalues():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "R, model = scripts.train_model(config, train_data, val_data)\n",
    "metrics = torchmetrics.MetricTracker(torchmetrics.MetricCollection({ \"R_cellwise_residuals\": torchmetrics.PearsonCorrCoef(num_outputs=1), \"R_cellwise\": torchmetrics.PearsonCorrCoef(num_outputs=1), \"MSE\": torchmetrics.MeanSquaredError() }))\n",
    "\n",
    "device = torch.device(config[\"env\"][\"device\"])\n",
    "metrics.to(device)\n",
    "\n",
    "test_dataloader = DataLoader( dataset=test_data, batch_size=config[\"optimizer\"][\"batch_size\"], shuffle=True, drop_last=True, collate_fn=scripts.custom_collate_fn\n",
    ")\n",
    "\n",
    "final_metrics = scripts.evaluate_step(model, test_dataloader, metrics, device)\n",
    "\n",
    "print(\"\\n\\n=== Final Test Metrics ===\") \n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba1b33-1b99-4af8-a89f-643debdb39f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436e6b9-6f81-4acd-a8a5-38933a9e6278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d0cd9-25df-4421-8460-a3bdabb34107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1bc664-b6e2-4201-8125-19618031a705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cfe888-453c-406d-94b6-634d8e5facd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26b8a8-c97f-4f1c-a69d-815b5ae18413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN3 - final",
   "language": "python",
   "name": "gnn3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
